==== Bambi Bayesian Mixed Effects Model ====
LLM: gpt41
Family: Bernoulli (logit link)
Formula: accuracy ~ language + question_type + question_tran + few + cot + mul + Temperature + max_tokens + top_p + presence_penalty + (1|question_id) + (1|augmentation)

---- Fixed Effects (posterior summary) ----
(Sorted by |mean|, descending; Intercept first)
                        mean     sd  hdi_3%  hdi_97%
Intercept             -1.696  0.454  -2.544   -0.854
cot[1]                -1.650  0.048  -1.740   -1.560
max_tokens[4000]       0.934  0.056   0.829    1.038
language[yy]           0.633  0.079   0.484    0.777
language[ey]           0.461  0.074   0.321    0.601
question_type[1]      -0.426  0.044  -0.509   -0.343
language[fy]           0.372  0.073   0.233    0.508
language[zw]           0.311  0.072   0.174    0.442
max_tokens[100]        0.308  0.055   0.201    0.406
few[1]                 0.291  0.045   0.207    0.373
mul[1]                 0.227  0.044   0.143    0.308
presence_penalty[0.5]  0.187  0.054   0.087    0.289
language[ry]           0.186  0.078   0.043    0.336
presence_penalty[1.5]  0.131  0.055   0.024    0.231
top_p[0.6]             0.078  0.054  -0.023    0.179
Temperature[2.0]       0.075  0.054  -0.025    0.178
question_tran[1]      -0.027  0.044  -0.111    0.056
Temperature[1.0]       0.015  0.053  -0.084    0.112
top_p[1.0]             0.003  0.053  -0.094    0.108

---- Random Effects SD (posterior summary) ----
(Sorted by mean, descending)
                       mean     sd  hdi_3%  hdi_97%
1|augmentation_sigma  1.208  0.398   0.626    1.947
1|question_id_sigma   0.441  0.066   0.325    0.565

---- Sampling diagnostics ----
(Sorted by reliability: r_hat closest to 1.0, then ess_bulk highest)
                       mcse_mean  mcse_sd  ess_bulk  ess_tail  r_hat
question_type[1]           0.000    0.000    9317.0    6519.0    1.0
cot[1]                     0.001    0.001    8332.0    5941.0    1.0
mul[1]                     0.000    0.000    8287.0    5587.0    1.0
few[1]                     0.001    0.001    8036.0    5996.0    1.0
question_tran[1]           0.001    0.001    7498.0    6030.0    1.0
Temperature[1.0]           0.001    0.000    6752.0    6659.0    1.0
Temperature[2.0]           0.001    0.001    6747.0    6137.0    1.0
top_p[0.6]                 0.001    0.001    6486.0    6018.0    1.0
presence_penalty[0.5]      0.001    0.001    6148.0    6230.0    1.0
top_p[1.0]                 0.001    0.001    6071.0    5647.0    1.0
max_tokens[100]            0.001    0.001    5878.0    6053.0    1.0
max_tokens[4000]           0.001    0.001    5783.0    6005.0    1.0
presence_penalty[1.5]      0.001    0.001    5662.0    5815.0    1.0
language[ry]               0.001    0.001    4229.0    5494.0    1.0
language[ey]               0.001    0.001    4012.0    5330.0    1.0
language[yy]               0.001    0.001    3974.0    5544.0    1.0
language[fy]               0.001    0.001    3912.0    5450.0    1.0
language[zw]               0.001    0.001    3884.0    5755.0    1.0
1|question_id[21]          0.003    0.002    2810.0    4839.0    1.0
1|question_id[6]           0.003    0.001    2586.0    4166.0    1.0
1|question_id[20]          0.003    0.002    2485.0    4234.0    1.0
1|question_id[22]          0.003    0.002    2483.0    3989.0    1.0
1|question_id[16]          0.003    0.002    2482.0    3990.0    1.0
1|question_id[10]          0.003    0.001    2462.0    4415.0    1.0
1|question_id[7]           0.003    0.002    2429.0    4851.0    1.0
1|question_id[5]           0.003    0.002    2416.0    4205.0    1.0
1|question_id[30]          0.003    0.001    2405.0    4603.0    1.0
1|question_id[4]           0.003    0.002    2396.0    4093.0    1.0
1|question_id[14]          0.003    0.001    2386.0    5007.0    1.0
1|question_id[25]          0.003    0.002    2369.0    4456.0    1.0
1|question_id[19]          0.003    0.002    2367.0    4061.0    1.0
1|question_id[27]          0.003    0.002    2363.0    4596.0    1.0
1|question_id[2]           0.003    0.002    2353.0    4299.0    1.0
1|question_id[1]           0.003    0.002    2288.0    3908.0    1.0
1|question_id[12]          0.003    0.001    2267.0    3957.0    1.0
1|question_id[18]          0.003    0.002    2240.0    3786.0    1.0
1|question_id[11]          0.003    0.001    2205.0    4182.0    1.0
1|question_id[3]           0.003    0.002    2203.0    3891.0    1.0
1|question_id[26]          0.003    0.002    2202.0    4185.0    1.0
1|question_id[29]          0.003    0.001    2184.0    4151.0    1.0
1|question_id[24]          0.003    0.002    2160.0    4080.0    1.0
1|question_id[28]          0.003    0.002    2159.0    3870.0    1.0
1|question_id[13]          0.003    0.001    2124.0    4301.0    1.0
1|question_id[17]          0.003    0.001    2102.0    4616.0    1.0
1|question_id[8]           0.003    0.001    2096.0    4175.0    1.0
1|question_id[9]           0.003    0.001    2091.0    4452.0    1.0
1|augmentation_sigma       0.009    0.011    2082.0    3107.0    1.0
1|question_id[23]          0.003    0.002    2045.0    3810.0    1.0
1|question_id[15]          0.003    0.002    1974.0    4141.0    1.0
1|question_id_sigma        0.002    0.001    1335.0    2616.0    1.0
1|augmentation[3]          0.014    0.013    1043.0    1941.0    1.0
1|augmentation[1]          0.014    0.013    1040.0    1998.0    1.0
1|augmentation[5]          0.014    0.013    1036.0    1945.0    1.0
1|augmentation[2]          0.014    0.013    1034.0    2030.0    1.0
1|augmentation[4]          0.014    0.014    1015.0    1868.0    1.0
1|augmentation[0]          0.014    0.014    1007.0    1827.0    1.0
Intercept                  0.014    0.013     997.0    2151.0    1.0
1|augmentation[7]          0.014    0.014     997.0    1828.0    1.0
1|augmentation[6]          0.014    0.014     996.0    1936.0    1.0
