==== Bambi Bayesian Mixed Effects Model ====
LLM: mistralL
Family: Bernoulli (logit link)
Formula: accuracy ~ language + question_type + question_tran + few + cot + mul + Temperature + max_tokens + top_p + presence_penalty + (1|question_id) + (1|augmentation)

---- Fixed Effects (posterior summary) ----
(Sorted by |mean|, descending; Intercept first)
                        mean     sd  hdi_3%  hdi_97%
Intercept             -0.683  0.424  -1.477    0.105
cot[1]                -1.731  0.046  -1.818   -1.644
question_type[1]      -0.861  0.044  -0.942   -0.777
max_tokens[4000]       0.496  0.053   0.391    0.589
language[zw]           0.373  0.071   0.240    0.504
language[yy]           0.282  0.077   0.141    0.434
language[ey]           0.237  0.073   0.100    0.372
mul[1]                 0.212  0.043   0.135    0.296
few[1]                 0.183  0.042   0.108    0.267
language[fy]           0.157  0.072   0.021    0.290
max_tokens[100]        0.154  0.052   0.058    0.252
language[ry]           0.112  0.076  -0.033    0.251
presence_penalty[0.5]  0.078  0.053  -0.019    0.181
Temperature[1.0]       0.046  0.051  -0.046    0.146
presence_penalty[1.5]  0.044  0.052  -0.057    0.139
top_p[0.6]             0.038  0.052  -0.060    0.134
Temperature[2.0]      -0.030  0.053  -0.130    0.070
top_p[1.0]             0.022  0.051  -0.072    0.120
question_tran[1]       0.006  0.043  -0.076    0.085

---- Random Effects SD (posterior summary) ----
(Sorted by mean, descending)
                       mean     sd  hdi_3%  hdi_97%
1|augmentation_sigma  1.136  0.371   0.580    1.805
1|question_id_sigma   0.497  0.072   0.374    0.637

---- Sampling diagnostics ----
(Sorted by reliability: r_hat closest to 1.0, then ess_bulk highest)
                       mcse_mean  mcse_sd  ess_bulk  ess_tail  r_hat
question_type[1]           0.000    0.001   12270.0    6053.0    1.0
mul[1]                     0.000    0.000   11565.0    6397.0    1.0
cot[1]                     0.000    0.001   10961.0    5646.0    1.0
few[1]                     0.000    0.001   10179.0    5911.0    1.0
question_tran[1]           0.000    0.000    9910.0    5908.0    1.0
top_p[0.6]                 0.001    0.001    9654.0    6012.0    1.0
max_tokens[100]            0.001    0.001    9542.0    6589.0    1.0
top_p[1.0]                 0.001    0.001    9237.0    6319.0    1.0
max_tokens[4000]           0.001    0.001    8831.0    5565.0    1.0
presence_penalty[0.5]      0.001    0.001    8366.0    5859.0    1.0
presence_penalty[1.5]      0.001    0.001    8048.0    6147.0    1.0
Temperature[1.0]           0.001    0.001    7912.0    6128.0    1.0
Temperature[2.0]           0.001    0.001    6578.0    5984.0    1.0
language[ey]               0.001    0.001    4852.0    5381.0    1.0
language[ry]               0.001    0.001    4679.0    5439.0    1.0
language[yy]               0.001    0.001    4508.0    5094.0    1.0
language[fy]               0.001    0.001    4243.0    5213.0    1.0
language[zw]               0.001    0.001    4235.0    5513.0    1.0
1|question_id[7]           0.003    0.002    2302.0    4092.0    1.0
1|augmentation_sigma       0.008    0.008    2266.0    3695.0    1.0
1|question_id[20]          0.003    0.002    2131.0    4329.0    1.0
1|question_id[5]           0.003    0.002    2051.0    3914.0    1.0
1|question_id[30]          0.003    0.002    2002.0    4114.0    1.0
1|question_id[19]          0.003    0.002    1989.0    3328.0    1.0
1|question_id[1]           0.003    0.002    1984.0    3526.0    1.0
1|question_id[4]           0.003    0.002    1976.0    3830.0    1.0
1|question_id[6]           0.003    0.002    1968.0    3746.0    1.0
1|question_id[13]          0.003    0.002    1964.0    3841.0    1.0
1|question_id[23]          0.003    0.002    1943.0    3944.0    1.0
1|question_id[26]          0.003    0.002    1900.0    4164.0    1.0
1|question_id[18]          0.003    0.002    1867.0    3372.0    1.0
1|question_id[11]          0.003    0.002    1866.0    4119.0    1.0
1|question_id[25]          0.003    0.002    1824.0    3642.0    1.0
1|question_id[2]           0.003    0.002    1821.0    3416.0    1.0
1|question_id[16]          0.003    0.002    1816.0    3155.0    1.0
1|question_id[8]           0.003    0.002    1812.0    3959.0    1.0
1|question_id[15]          0.003    0.001    1809.0    3870.0    1.0
1|question_id[3]           0.003    0.002    1808.0    3523.0    1.0
1|question_id[21]          0.003    0.002    1808.0    3709.0    1.0
1|question_id[29]          0.003    0.002    1801.0    3673.0    1.0
1|question_id[24]          0.003    0.002    1781.0    4205.0    1.0
1|question_id[28]          0.004    0.002    1766.0    3693.0    1.0
1|question_id[10]          0.003    0.002    1755.0    4115.0    1.0
1|question_id[12]          0.003    0.002    1753.0    3624.0    1.0
1|augmentation[1]          0.010    0.007    1744.0    3040.0    1.0
1|augmentation[2]          0.010    0.007    1727.0    2747.0    1.0
1|question_id[17]          0.003    0.002    1721.0    3842.0    1.0
1|augmentation[5]          0.010    0.007    1720.0    3049.0    1.0
1|augmentation[3]          0.010    0.007    1709.0    2703.0    1.0
1|question_id[27]          0.003    0.002    1701.0    3645.0    1.0
1|augmentation[4]          0.010    0.007    1698.0    2794.0    1.0
Intercept                  0.010    0.008    1695.0    2696.0    1.0
1|question_id[22]          0.004    0.002    1687.0    3693.0    1.0
1|augmentation[0]          0.010    0.008    1683.0    2868.0    1.0
1|question_id[9]           0.003    0.002    1674.0    4052.0    1.0
1|augmentation[7]          0.010    0.007    1667.0    2818.0    1.0
1|augmentation[6]          0.010    0.007    1659.0    2847.0    1.0
1|question_id[14]          0.004    0.002    1616.0    3946.0    1.0
1|question_id_sigma        0.002    0.001    1539.0    2744.0    1.0
