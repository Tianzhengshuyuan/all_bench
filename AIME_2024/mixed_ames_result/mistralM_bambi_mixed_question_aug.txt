==== Bambi Bayesian Mixed Effects Model ====
LLM: mistralM
Family: Bernoulli (logit link)
Formula: accuracy ~ language + question_type + question_tran + few + cot + mul + Temperature + max_tokens + top_p + presence_penalty + (1|question_id) + (1|augmentation)

---- Fixed Effects (posterior summary) ----
(Sorted by |mean|, descending; Intercept first)
                        mean     sd  hdi_3%  hdi_97%
Intercept             -1.420  0.452  -2.222   -0.547
max_tokens[4000]       2.091  0.059   1.977    2.199
cot[1]                -1.633  0.048  -1.722   -1.544
question_type[1]      -0.943  0.046  -1.025   -0.853
language[yy]           0.628  0.080   0.475    0.773
language[ey]           0.437  0.074   0.305    0.582
max_tokens[100]        0.432  0.057   0.325    0.541
language[zw]           0.421  0.074   0.277    0.552
language[fy]           0.327  0.074   0.189    0.466
language[ry]           0.198  0.078   0.049    0.345
few[1]                 0.197  0.046   0.113    0.286
mul[1]                 0.177  0.044   0.098    0.263
presence_penalty[0.5]  0.110  0.055   0.000    0.211
presence_penalty[1.5]  0.095  0.055  -0.005    0.203
question_tran[1]      -0.079  0.045  -0.161    0.006
Temperature[1.0]      -0.039  0.053  -0.138    0.061
top_p[0.6]            -0.021  0.055  -0.123    0.086
top_p[1.0]            -0.013  0.054  -0.113    0.087
Temperature[2.0]       0.011  0.055  -0.092    0.114

---- Random Effects SD (posterior summary) ----
(Sorted by mean, descending)
                       mean     sd  hdi_3%  hdi_97%
1|augmentation_sigma  1.236  0.409   0.650    1.972
1|question_id_sigma   0.627  0.092   0.473    0.806

---- Sampling diagnostics ----
(Sorted by reliability: r_hat closest to 1.0, then ess_bulk highest)
                       mcse_mean  mcse_sd  ess_bulk  ess_tail  r_hat
question_type[1]           0.000    0.001   10142.0    5944.0   1.00
mul[1]                     0.000    0.001    9404.0    6205.0   1.00
cot[1]                     0.000    0.001    9181.0    6295.0   1.00
question_tran[1]           0.000    0.000    8806.0    5712.0   1.00
few[1]                     0.001    0.001    8064.0    6074.0   1.00
Temperature[2.0]           0.001    0.001    7592.0    5987.0   1.00
Temperature[1.0]           0.001    0.001    7475.0    6133.0   1.00
presence_penalty[0.5]      0.001    0.001    6793.0    5922.0   1.00
presence_penalty[1.5]      0.001    0.001    6625.0    6536.0   1.00
top_p[1.0]                 0.001    0.001    6064.0    5939.0   1.00
max_tokens[4000]           0.001    0.001    5990.0    5678.0   1.00
top_p[0.6]                 0.001    0.001    5737.0    5779.0   1.00
max_tokens[100]            0.001    0.001    5709.0    5796.0   1.00
language[yy]               0.001    0.001    4202.0    4975.0   1.00
language[ry]               0.001    0.001    4107.0    5370.0   1.00
language[zw]               0.001    0.001    3813.0    5078.0   1.00
language[fy]               0.001    0.001    3803.0    5387.0   1.00
language[ey]               0.001    0.001    3794.0    5191.0   1.00
1|augmentation_sigma       0.009    0.011    1988.0    3107.0   1.00
1|question_id[5]           0.004    0.002    1464.0    3004.0   1.00
1|question_id[20]          0.005    0.002    1461.0    3359.0   1.00
1|question_id[4]           0.005    0.002    1401.0    2911.0   1.00
1|question_id[19]          0.005    0.002    1363.0    2763.0   1.00
1|question_id[7]           0.005    0.002    1356.0    3128.0   1.00
1|question_id[6]           0.004    0.002    1356.0    2667.0   1.00
1|question_id[13]          0.004    0.002    1347.0    3068.0   1.00
1|question_id[9]           0.004    0.002    1335.0    2981.0   1.00
1|question_id[21]          0.004    0.002    1333.0    3166.0   1.00
1|question_id[10]          0.005    0.002    1330.0    2812.0   1.00
1|question_id[8]           0.004    0.002    1318.0    2629.0   1.00
1|question_id[17]          0.005    0.002    1291.0    3316.0   1.00
1|question_id[11]          0.005    0.002    1288.0    2892.0   1.00
1|question_id[22]          0.005    0.002    1280.0    3168.0   1.00
1|question_id[24]          0.004    0.002    1278.0    2661.0   1.00
1|question_id[23]          0.005    0.002    1269.0    2902.0   1.00
1|question_id[28]          0.005    0.002    1261.0    3167.0   1.00
1|question_id[25]          0.005    0.002    1258.0    3165.0   1.00
1|question_id[14]          0.005    0.002    1244.0    3029.0   1.00
1|question_id[1]           0.004    0.002    1241.0    2942.0   1.00
1|question_id[15]          0.005    0.002    1240.0    3043.0   1.00
1|question_id[27]          0.005    0.002    1235.0    2748.0   1.00
1|question_id[3]           0.005    0.002    1233.0    3083.0   1.00
1|question_id[2]           0.005    0.002    1230.0    2847.0   1.00
1|question_id[26]          0.005    0.002    1224.0    2983.0   1.00
1|question_id[30]          0.005    0.002    1222.0    3046.0   1.00
1|question_id[18]          0.005    0.002    1213.0    3076.0   1.00
1|question_id[16]          0.005    0.002    1204.0    3096.0   1.00
Intercept                  0.013    0.008    1202.0    1998.0   1.00
1|augmentation[3]          0.013    0.007    1201.0    2356.0   1.00
1|augmentation[1]          0.013    0.008    1195.0    2127.0   1.00
1|question_id[12]          0.005    0.002    1194.0    2961.0   1.00
1|question_id[29]          0.005    0.002    1194.0    2801.0   1.00
1|augmentation[5]          0.013    0.008    1174.0    2047.0   1.00
1|augmentation[0]          0.013    0.008    1169.0    1914.0   1.00
1|augmentation[2]          0.013    0.008    1166.0    2074.0   1.00
1|augmentation[4]          0.013    0.007    1160.0    2168.0   1.00
1|augmentation[6]          0.013    0.008    1154.0    1995.0   1.00
1|augmentation[7]          0.013    0.008    1145.0    1894.0   1.00
1|question_id_sigma        0.003    0.002     888.0    2038.0   1.01
