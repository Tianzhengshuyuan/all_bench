==== Bambi Bayesian Mixed Effects Model ====
LLM: qwen25
Family: Bernoulli (logit link)
Formula: accuracy ~ language + question_type + question_tran + few + cot + mul + Temperature + max_tokens + top_p + presence_penalty + (1|question_id) + (1|augmentation)

---- Fixed Effects (posterior summary) ----
(Sorted by |mean|, descending; Intercept first)
                        mean     sd  hdi_3%  hdi_97%
Intercept             -0.580  0.479  -1.502    0.296
cot[1]                -2.336  0.054  -2.438   -2.237
question_type[1]      -1.274  0.047  -1.363   -1.185
max_tokens[4000]       0.539  0.057   0.430    0.645
language[zw]           0.409  0.076   0.264    0.548
few[1]                 0.364  0.046   0.278    0.450
language[ey]           0.358  0.076   0.218    0.501
language[fy]           0.324  0.075   0.185    0.465
max_tokens[100]        0.210  0.056   0.104    0.315
mul[1]                 0.193  0.046   0.108    0.279
language[yy]           0.130  0.083  -0.027    0.284
language[ry]           0.120  0.081  -0.030    0.272
presence_penalty[0.5]  0.116  0.055   0.011    0.217
presence_penalty[1.5]  0.095  0.055  -0.008    0.197
top_p[0.6]             0.071  0.056  -0.037    0.172
question_tran[1]       0.036  0.046  -0.048    0.121
Temperature[2.0]      -0.034  0.056  -0.137    0.072
Temperature[1.0]      -0.029  0.054  -0.130    0.074
top_p[1.0]            -0.013  0.055  -0.114    0.092

---- Random Effects SD (posterior summary) ----
(Sorted by mean, descending)
                       mean     sd  hdi_3%  hdi_97%
1|augmentation_sigma  1.325  0.443   0.693    2.161
1|question_id_sigma   0.524  0.078   0.390    0.675

---- Sampling diagnostics ----
(Sorted by reliability: r_hat closest to 1.0, then ess_bulk highest)
                       mcse_mean  mcse_sd  ess_bulk  ess_tail  r_hat
few[1]                     0.000    0.001    9736.0    6214.0   1.00
cot[1]                     0.001    0.001    8821.0    6198.0   1.00
question_tran[1]           0.000    0.001    8488.0    5998.0   1.00
mul[1]                     0.001    0.001    8214.0    6471.0   1.00
question_type[1]           0.001    0.001    7858.0    6059.0   1.00
max_tokens[4000]           0.001    0.001    7169.0    6407.0   1.00
max_tokens[100]            0.001    0.001    6715.0    6104.0   1.00
top_p[1.0]                 0.001    0.001    6618.0    6070.0   1.00
presence_penalty[1.5]      0.001    0.001    6245.0    5910.0   1.00
Temperature[2.0]           0.001    0.001    6044.0    6014.0   1.00
Temperature[1.0]           0.001    0.001    5865.0    5773.0   1.00
presence_penalty[0.5]      0.001    0.001    5717.0    6060.0   1.00
top_p[0.6]                 0.001    0.001    5698.0    5894.0   1.00
language[yy]               0.001    0.001    3627.0    4856.0   1.00
language[ey]               0.001    0.001    3543.0    4552.0   1.00
language[ry]               0.001    0.001    3504.0    4978.0   1.00
language[zw]               0.001    0.001    3455.0    4888.0   1.00
language[fy]               0.001    0.001    3372.0    5405.0   1.00
1|question_id[20]          0.004    0.002    2234.0    3924.0   1.00
1|augmentation_sigma       0.010    0.011    2000.0    2845.0   1.00
1|question_id[7]           0.004    0.002    1956.0    3707.0   1.00
1|question_id[22]          0.004    0.002    1875.0    3768.0   1.00
1|question_id[12]          0.004    0.002    1867.0    3802.0   1.00
1|question_id[5]           0.004    0.002    1842.0    3430.0   1.00
1|question_id[16]          0.004    0.002    1827.0    4074.0   1.00
1|question_id[4]           0.004    0.002    1815.0    3398.0   1.00
1|question_id[30]          0.004    0.002    1815.0    3413.0   1.00
1|question_id[26]          0.004    0.002    1802.0    2970.0   1.00
1|question_id[11]          0.004    0.002    1771.0    3628.0   1.00
1|question_id[9]           0.004    0.002    1769.0    3324.0   1.00
1|question_id[17]          0.004    0.002    1758.0    3561.0   1.00
1|question_id[15]          0.004    0.002    1749.0    3216.0   1.00
1|question_id[23]          0.004    0.002    1745.0    3547.0   1.00
1|question_id[13]          0.004    0.002    1734.0    3150.0   1.00
1|question_id[21]          0.004    0.002    1729.0    3594.0   1.00
1|question_id[8]           0.004    0.002    1717.0    3758.0   1.00
1|question_id[2]           0.004    0.002    1709.0    3691.0   1.00
1|question_id[25]          0.004    0.002    1709.0    3173.0   1.00
1|question_id[3]           0.004    0.002    1696.0    3486.0   1.00
1|question_id[28]          0.004    0.002    1689.0    3334.0   1.00
1|question_id[10]          0.004    0.002    1687.0    3411.0   1.00
1|question_id[27]          0.004    0.002    1674.0    3153.0   1.00
1|question_id[19]          0.004    0.002    1673.0    3036.0   1.00
1|question_id[29]          0.004    0.002    1664.0    3321.0   1.00
1|question_id[24]          0.004    0.002    1654.0    3198.0   1.00
1|question_id[14]          0.004    0.002    1648.0    3231.0   1.00
1|question_id[18]          0.004    0.002    1641.0    3186.0   1.00
1|question_id[6]           0.004    0.002    1617.0    3700.0   1.00
1|question_id[1]           0.004    0.002    1607.0    3470.0   1.00
1|question_id_sigma        0.002    0.001    1309.0    2253.0   1.00
1|augmentation[1]          0.014    0.009    1124.0    2220.0   1.00
1|augmentation[5]          0.014    0.009    1109.0    2251.0   1.00
1|augmentation[3]          0.014    0.009    1108.0    2220.0   1.00
1|augmentation[2]          0.014    0.009    1090.0    2163.0   1.00
1|augmentation[0]          0.014    0.009    1083.0    2097.0   1.00
1|augmentation[4]          0.014    0.009    1073.0    2269.0   1.00
1|augmentation[6]          0.014    0.009    1069.0    2143.0   1.00
1|augmentation[7]          0.014    0.009    1068.0    2159.0   1.00
Intercept                  0.015    0.009    1071.0    2062.0   1.01
