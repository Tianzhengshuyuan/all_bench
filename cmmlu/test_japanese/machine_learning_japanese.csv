二値分類問題において、テストセットの正例と負例の数が不均衡なときに、以下の評価指標の中で相対的に不合理なものはどれですか？（ただし、precision=TP/(TP+FP)、recall=TP/(TP+FN)とします。）,F-値: 2*recall*precision/(recall+precision),G-mean: sqrt(precision*recall),正解率: (TP+TN)/全体,AUC: ROC曲線下面積,C
過学習を防ぐために、以下のどの対策が適切でないか,ドロップアウト層を追加する,層の数を深くする,データ拡張を行う,正則化項を追加する,B
データセットがあり、深さが6の決定木を使用して100%の精度で訓練できると仮定します。以下の2つの点を考慮し、正しい選択肢を選んでください。1. 深さが4の場合、高バイアス・低分散となる；2. 深さが4の場合、低バイアス・低分散となる。なお、他のすべてのハイパーパラメータは同一であり、他の要因には影響がないものとします。,1 と 2,2 のみ,どちらもない,1 のみ,D
次のうち、高次元データの次元削減に利用できない方法はどれですか？,LASSO,バギング,主成分分析法,クラスター分析,B
機械学習において、L1正則化とL2正則化の違いは次のうちどれか。,L1を使用すると、スパースで平滑な重みが得られる,L2を使用すると、スパースで平滑な重みが得られる,L1を使用するとスパースな重みが、L2を使用すると平滑な重みが得られる,L2を使用するとスパースな重みが、L1を使用すると平滑な重みが得られる,C
隠れマルコフモデルと条件付き確率場モデルに関する次の記述のうち、誤っているものはどれか？,隠れマルコフモデルは固有子認識、形態素解析および品詞タグ付けのタスクに使用できる,隠れマルコフモデルと（訳注：原文の「隠馬模型和隐马模型」は重複しているように見える）はどちらも生成モデルである,隠れマルコフモデルは確率無向グラフモデルではない,特徴の選定と最適化は隠れマルコフモデルの結果に大きな影響を与える,B
以下のどのハイパーパラメータを増加させると、ランダムフォレストで過学習が発生する可能性がありますか？,学習率,木の数,木の深さ,上記のいずれでもない,C
次のうち、反復二分木3世代アルゴリズムに関する誤った説明はどれですか。,反復二分木3世代アルゴリズムは二分木モデルです,情報ゲインの計算にはGINI係数ではなくエントロピーを使用することができます,反復二分木3世代アルゴリズムでは特徴量を離散化する必要があります,情報ゲインが最大の特徴を選択し、木のルートノードとします,A
以下のどの選択肢がK分割交差検証について正しく述べていますか？,K=N の場合、これは留一交差検証と呼ばれます。N は検証セット内のサンプル数です,小さい K 値と比較して、より大きい K 値の方が交差検証の構造に対して高い信頼を得られます,上記すべて,K を大きくすると、交差検証の結果を得るためにより多くの時間がかかるようになります,C
m個のサンプルが存在し、n（n<=m）回のサンプリングを行う場合、bootstrapデータとは何か？,全部のN個のサンプルから、復元せずにn個のサンプルを抽出する,全部のM個の特徴から、復元せずにm個の特徴を抽出する,全部のN個のサンプルから、復元しながらn個のサンプルを抽出する,全部のM個の特徴から、復元しながらm個の特徴を抽出する,C
もし特徴ベクトルの相関係数をパターン類似度の測定基準とする場合、クラスタリングアルゴリズムの結果に主に影響を与える要因には次のものがある。,次元,既知カテゴリサンプルの品質,上記のいずれでもない,分類基準,D
モデルの訓練過程において、一般的にデータを以下のセットに分けます。,検証セット,テストセット,トレーニングセット,上記すべての選択肢が可能,D
L1正則化を使用したロジスティック回帰で二値分類を行っています。ここで、C は正則化パラメータであり、w1 と w2 はそれぞれ x1 と x2 の係数です。C の値を 0 から非常に大きな値に増やしていったとき、次のうち正しい選択肢はどれでしょうか。,最初に w1 が 0 となり、その後で w2 も 0 になる,w1 と w2 が同時に 0 になる,最初に w2 が 0 となり、その後で w1 も 0 になる,C が非常に大きな値になっても、w1 と w2 はどちらも 0 にはならない,D
次の方法の中で、特徴量の次元削減に使用できないもの包括えます。,深層学習 SparseAutoEncoder,行列の特異値分解 SVD,線形判別分析,主成分分析,A
ベイズの定理で求められる確率はどれですか？,事前確率,上記以外,条件付き確率,結合確率,C
機械学習において、学習器の汎化性能を解釈する際に頻繁に用いられるバイアス-バリアンス分解について、以下のうち誤っている記述はどれか。,バリアンスは学習器の予測の安定性を表す。,バイアスは学習器の予測の正確さを表す。,汎化性能は学習アルゴリズムの能力、データの十分性、および学習タスク自体の難易度によって共に決定される。,バリアンスとは予測値の期待値と真の値とのずれを指す。,D
もしロジスティック回帰アルゴリズムを使ってパソコンの販売台数を予測しようとしたとします。新しいテストセットで自分の仮説を検証したところ、予測値に大きな偏りがあることが分かりました。また、自分の仮説はトレーニングセットでもあまり良い結果を出していません。このような場合、以下のどの対策を避けるべきでしょうか。,正則化項 λ を小さくしてみる,クロス項（交叉特徴）を増やしてみる,サンプル数を増やす,より小さなテストセットまたは特徴量を使用してみる,D
P(w) を語 w の確率を表すとします。ここで、P(南京) = 0.8、P(市長) = 0.6、P(江大橋) = 0.4、P(南京市) = 0.3、P(長江大橋) = 0.5 であると仮定します。もしも前後の語の出現が独立していると仮定すると、形態素解析結果は次のようになります。,南京_市長_江大橋,南京市_長江_大橋,南京市長_江大橋,南京市_長江大橋,A
ロジスティック回帰と一般的な回帰分析の違いは何か,ロジスティック回帰はイベントの可能性を予測するために設計されている,ロジスティック回帰を使用して回帰係数を推定できる,上記のすべて,ロジスティック回帰を使用してモデルの適合度を測定できる,C
以下のうち、「タイプ 1 (Type-1)」および「タイプ 2 (Type-2)」エラーに関する誤った記述はどれですか。,タイプ 1 エラーは、仮説が正しい場合にそれを棄却したときに発生する,タイプ 1 は一般的に偽陽性と呼ばれるが、タイプ 2 は一般的に偽陰性と呼ばれる,上記すべて,タイプ 2 は一般的に偽陽性と呼ばれるが、タイプ 1 は一般的に偽陰性と呼ばれる,D
基本K-平均化法に影響を与える主な要因には次のものがある。,初期クラスタ中心の選定,クラスタリング基準,サンプル入力順序,パターン類似度測定,D
ガウシアン混合モデル (GMM) はどのようなモデルですか？,教師なし学習モデル,上記のいずれでもない,半教師あり学習モデル,教師あり学習モデル,A
文法規則に基づく方法は,条件付き確率場,最大エントロピーモデル,構文・意味解析,最大エントロピー隠れマルコフモデル,B
"正のサンプルである最初の点の特徴ベクトルは (0, -1) であり、負のサンプルである2番目の点の特徴ベクトルは (2, 3) です。この2つのサンプル点からなる訓練データを使って構築した線形SVM分類器の分離面の方程式は次のうちどれか。",2x-y=0,x+2y=5,x+2y=3,2x+y=4,C
隠れマルコフモデルがあり、その観測値空間は、状態空間は である。ビタビアルゴリズム（Viterbi algorithm）を用いて復号を行う場合、時間計算量は次のようになる。,O(NK),O(N^2K),上記のいずれでもない,O(NK^2),C
あなたが非常に大きなγ値を持つRBFカーネルを使用したと仮定します。これは次を意味します：,モデルは超平面からの点の距離に影響されない,上記のいずれでもない,モデルは超平面に近い点のみを使用してモデル化する,モデルは超平面から遠い点も使用してモデル化する,C
ARMA（自己回帰移動平均モデル）、AR（自己回帰モデル）、MA（移動平均モデル）のモデルのパワースペクトルに関して、次のうち正しい記述はどれか。,ARモデルはゼロ点が単位円に近づくとき、ARスペクトルは鋭いピークを持つ,MAモデルは同一のオールパスフィルタによって生成される,MAモデルは極点が単位円に近づくとき、MAスペクトルは深い谷を持つ,RMAスペクトルは鋭いピークと深い谷の両方を持つ,D
変数選択は、最適な識別器の部分集合を選択するために使用されます。モデルの効率性を考慮する場合、次のうちで変数選択の検討事項として不適切なものはどれでしょうか。,交差検証,変数がモデルの説明においてどの程度重要であるか,特徴が持つ情報量,複数の変数が実際には同じ用途を持っていること,B
次の時系列モデルの中で、どれがボラティリティの分析と予測を比較的適切に適合できるか。,自己回帰モデル ARモデル,自己回帰移動平均モデル,移動平均モデル,一般化自己回帰移動平均モデル,D
"線形モデルを構築する際には、変数間の相関性に注意を払います。相関行列で相関係数を調べるとき、3組の変数 (Var1 と Var2, Var2 と Var3, Var3 と Var1) の相関係数がそれぞれ -0.98、0.45、1.23 であることが分かった場合、次のうちどの結論を導くことができますか。",上記すべて,Var1 と Var2 は非常に相関しているので、どちらか一方を取り除くことができる,Var1 と Var2 は非常に相関している,Var3 と Var1 の相関係数 1.23 はあり得ない,A
特徴選択を行う際に使用される可能性のある方法は、機械学習において以下の通りです。,上記すべて,カイ二乗検定,情報利得,期待交差エントロピー,A
LSTMとGRUの主な違いの1つは、GRUがLSTMのどのゲートを統合したかです。,forget gate と input gate,input gate と output gate,forget gate と output gate,output gate と reset gate,A
記号セット a、b、c、d があり、それらは互いに独立で、それぞれの確率は 1/2、1/4、1/8、1/16 である。この中で情報量が最も小さい記号はどれか。,d,b,a,c,C
log-loss関数を評価基準として使用すると仮定します。以下の選択肢のうち、評価基準としてのlog-lossに関する正しい説明はどれですか。,上記すべて,log-lossが低ければ低いほどモデルは良い,特定の観測において、分類器が正しいカテゴリに非常に小さい確率を割り当てた場合、log-lossへの対応する寄与は非常に大きくなる,分類器がある誤った分類に対して高い信頼度を持つ場合、log-lossはそれを厳しく評価する,A
一組のデータの共分散行列Pが与えられたとき、主成分について以下の説明の中で誤っているものはどれか,主成分分析とはK-L変換のことである,主成分分解によって、共分散行列は対角行列となる,主成分分析の最適基準とは、一組のデータを一組の直交基底で展開し、同じ数の成分のみを採用する条件下で、平均二乗誤差によって打ち切り誤差が最小になることである,主成分は共分散行列の固有値を求めて得られる,A
アテンション機構に関する以下の記述のうち、誤っているものはどれですか。,アテンション機構は系列内の各要素に重み係数を割り当てます,アテンション機構は機械読解や質疑応答などの場面で利用できます,伝統的なエンコーダー・デコーダーモデルには長距離依存問題が存在します,アテンション機構の派生形である多頭アテンション機構は並列化に適していません。というのも、その各ステップの計算は前のステップの計算結果に依存するからです,D
過学習を抑える方法ではないものはどれですか？,より多くの訓練データを収集する,データのクリーニングを行い、ノイズを減らす,ニューロンネットワークの隠れ層のノード数を増やす,モデルの仮定を単純化する,C
"与えられた3つの変数 X、Y、Z について、(X, Y)、(Y, Z)、(X, Z) のピアソン相関係数はそれぞれ C1、C2、C3 である。ここで、X のすべての値に 2 を加算（つまり X+2）、Y のすべての値から 2 を減算（つまり Y-2）し、Z はそのままにする。この操作後の (X, Y)、(Y, Z)、(X, Z) の相関係数をそれぞれ D1、D2、D3 とする。では、D1、D2、D3 と C1、C2、C3 との関係はどうなるか。","D1 = C1, D2 < C2, D3 < C3","D1= C1, D2 < C2, D3 > C3","D1 = C1, D2 = C2, D3 = C3","D1 = C1, D2 > C2, D3 > C3",C
決定木において、分割ノードの情報ゲインに関して誤っている記述はどれか。,情報ゲインは値が多い属性を好む傾向がある,情報ゲインはエントロピーから算出できる,不純度が小さいノードほど全体を区別するために多くの情報を必要とする,上記のいずれでもない,C
回帰分析における残差に関する次の記述のうち、正しいものはどれか,残差の平均値は常にゼロより小さい,残差の平均値は常にゼロより大きい,残差の平均値は常にゼロである,残差にはこのような規則性はない,C
我々は5000個の特徴量と100万件のデータからなる機械学習モデルを構築します。このようなビッグデータの訓練に対して、どうやって効果的に対処すべきでしょうか？,一部のサンプルをランダムに抽出し、それら少数のサンプルに基づいて訓練する,上記すべて,オンライン機械学習アルゴリズムを使用してみる,PCAアルゴリズムを適用して次元削減を行い、特徴量を減らす,B
次のうち、条件付き確率場モデルが隠れマルコフモデルや最大エントロピー隠れマルコフモデルに対して持つ利点に含まれないものはどれか,速度快,より多くの文脈情報を取り入れ可能,グローバル最適,特徴量の柔軟性,A
次のうち、辞書ベースの方法による中国語形態素解析の基本的な方法ではないものはどれですか。,最大エントロピーモデル,最大概率法,最長一致法,最短経路法,A
SVMを使用してデータXを学習していると仮定します。データXの中には誤りが含まれる点があります。ここで、二次の多項式カーネル関数（多項式の次数は2）を使用し、緩やかな変数Cをハイパーパラメータの1つとして使用しています。もし小さなC値（Cが0に近づく値）を使用した場合、次のようになります：,不確定,誤分類,正確な分類,上記のいずれでもない,B
以下のどの方法が生成モデルに属しますか,条件随場,伝統的なニューロンネットワーク,朴素ベイズ,線形回帰,C
他の条件が変わらないという前提で、次のうちどの方法が機械学習における過学習の問題を引き起こしやすくなるか。,訓練セットの数量を増やす,スパースな特徴を削除する,SVMアルゴリズムでガウシアンカーネル/RBFカーネルを使用する,ニューラルネットワークの隠れ層ノード数を減らす,C
勾配ブースティングツリーのアルゴリズムに関して、以下のうち正しい記述は次のとおりです。,最小サンプル分割数を増加させることで、過学習を抑えることができます。,個々の学習器を訓練するサンプル数を減少させることで、バイアスを低下させることができます。,最小サンプル分割数を増加させると、過学習が発生します。,個々の学習器を訓練するサンプル数を増加させることで、分散を低下させることができます。,A
SVMのトレーニング後、線形決定境界を得たが、このモデルがアンダーフィッティングしていると思う。次のイテレーションでモデルをトレーニングする際には、以下のうちどれを考慮すべきか。,トレーニングデータを減らす,特徴量を減らす,より多くの変数を計算する,トレーニングデータを増やす,C
音声信号はどのような特性を持っているため、窓関数による処理を施すことができるのでしょうか？,ランダム単調性,どれも該当しない,短時間定常性,単調不変性,C
k分割交差検証に関して、kの値について正しい記述は次のうちどれか。,大きなkを選択すれば、biasは小さくなる（訓練データが全体のデータセットに近づくため）,kが大きければ大きいほど良いというわけではない。大きなkは評価時間を増加させる可能性がある,kを選ぶ際には、データセット間の分散を最小化する必要がある,上記すべて,D
以下のうち、ユークリッド距離の特性に該当するものはどれか。,スケール変換不変性,回転不変性,単位系の影響を受けない特性,パターンの分布を考慮する,B
ナイーブベイズは特殊なベイズ分類器であり、特徴変数はX、クラスラベルはCである。その一つの仮定として、,特徴変数Xの各次元はクラス条件付き独立な確率変数であること,P(X|C)がガウス分布であること,平均0、標準偏差sqr(2)/2の正規分布であること,各クラスの事前確率P(C)が等しいこと,A
n次元空間において、外れ値（離群点）を検出するのに最も適した方法は次のうちどれか。,箱ひげ図を作成する,散布図を作成する,正規分布確率プロットを作成する,マハラノビス距離,D
線形回帰モデルにおいて、追加変数を含め以下のどれが正しいか,R-Squared は減少する、Adjusted R-squared も減少する,R-Squared は定数である、Adjusted R-squared は増加する,R-Squared も Adjusted R-squared も増加する,上記のいずれでもない,D
データサイエンティストは複数のアルゴリズム（モデル）を同時に使用して予測を行い、最後にそれらの結果を統合して最終的な予測を行うことがあります（アンサンブル学習）。以下のうち、アンサンブル学習に関する正しい記述はどれですか？,個々のモデル間には高い相関がある,個々のモデルはすべて同じアルゴリズムを使用している,個々のモデル間には低い相関がある,アンサンブル学習では「投票」よりも「平均重み」を使用した方が良い,C
大規模なデータセットで決定木を訓練したいが、より少ない時間で実行するために、以下のうちどれを行うことができますか？,学習率を増加させる,木の数を減少させる,木の深さを増加させる,木の深さを減少させる,D
データセットの特徴数を減らしたい、つまり次元削減を行いたいとします。適切な方法を選択してください。,すべての上記,まずすべての特徴を使用してモデルを訓練し、テストセットでの性能を得ます。その後、1つの特徴を除いて再度訓練し、交差検証によってテストセットでの性能を確認します。もし、その性能が以前より良ければ、その特徴を除去することができます。,前方特徴選択（Forward feature selection）および後方特徴排除（Backward feature elimination）の方法を使用する,相関行列を調べ、相関が最も高いいくつかの特徴を除去する,A
次の活性化関数のうち、勾配消失の問題を解決できないものはどれですか。,Leaky-Relu,Elu,Sigmoid,Relu,C
次のうち、一般的に使われるテキスト分類の特徴選択アルゴリズムに含まれないものはどれですか。,主成分分析,互情報,情報利得,カイ二乗検定値,A
フィッシャー線形判別関数の求解過程は、M次元特徴ベクトルを（ ）に投影して求解を行う。,一維空間,三維空間,M-1維空間,二維空間,A
XとYの間の较强な関係を示しているのは次のうちどれか,相関係数が0.9である,どれも正しくない,Beta係数が0であるという帰無仮説のp値が0.0001である,Beta係数が0であるという帰無仮説のt統計量が30である,A
k-平均化アルゴリズムにおいて、以下の選択肢のうちグローバル最小を得るために使えるのはどれですか？,上記すべて,クラスターの最適な数を見つけること,繰り返し回数を調整すること,異なるセントロイド初期値を使ってアルゴリズムを複数回実行すること,A
統計言語モデルでは、通常任意の文の可能性を確率の形式で記述し、最大尤度推定を用いて評価します。いくつかの低頻度の単語については、訓練データをどれだけ増やしても出現頻度が依然として非常に低いです。次のうち、この問題を解決できる方法はどれでしょうか？,データ平滑,N元文法,一元文法,一元切分,A
次の記述のうち、誤っているものはどれか。,n個のデータ点が与えられ、その半分が訓練に、半分がテストに使用される場合、訓練誤差とテスト誤差の差はnの増加に伴って小さくなる。,ブースティングとバギングはともに複数の分類器による投票を行う方法であり、どちらも個別の分類器の正解率に基づいて重みを決定する。,SVMはノイズ（例えば他の分布からのノイズサンプル）に対してロバスト性を持っている。,AdaBoostアルゴリズムでは、誤分類されたすべてのサンプルの重み更新比率は同じではない。,B
"あるバイナリソースXは、{-1,1}のシンボルセットを送信する。離散無記憶チャネルを通じて伝送されるため、受信端Yは{-1,1,0}のシンボルセットを受信する。ノイズの影響で、次のような確率が与えられている。P(x=-1)=1/4、P(x=1)=3/4、P(y=-1|x=-1)=4/5、P(y=0|x=-1)=1/5、P(y=1|x=1)=3/4、P(y=0|x=1)=1/4。このとき、条件エントロピーH(Y|X)を求めよ。",0.5372,0.2375,0.5273,0.3275,B
次のうち、データセットの次元を削減するのに最も適した技術はどれですか？,分散が大きい列を削除する,欠損値が多い列を削除する,異なるデータ傾向の列を削除する,どれでもない,B
次のうち、高次元データの次元削減に特に適していないものはどれですか。,クラスタリング分析,LASSO,ウェーブレット分析法,ラプラス固有マッピング,A
ロジスティック回帰と多変量回帰分析の違いはどれですか？,ロジスティック回帰の回帰係数の評価,ロジスティック回帰が特定のイベント発生の確率を予測する,ロジスティック回帰が高い当てはまりの良さを持つ,上記すべて,D
最も有名な次元削減アルゴリズムはPCAとt-SNEです。この2つのアルゴリズムをそれぞれデータ「X」に適用し、データセット「X_projected_PCA」と「X_projected_tSNE」を得ました。次のうち、「X_projected_PCA」と「X_projected_tSNE」について正しい記述はどれでしょうか。,両方とも最近傍空間で解釈できる,X_projected_PCA は最近傍空間で解釈できる,両方とも最近傍空間で解釈できない,X_projected_tSNE は最近傍空間で解釈できる,D
ビタビアルゴリズムに関する次の記述の中で誤っているものはどれか,ビタビアルゴリズムにおける遷移確率は、ある隠れ状態から別の隠れ状態に遷移する確率である,ビタビアルゴリズムは貪欲法の一種である,ビタビアルゴリズムは中国語形態素解析のタスクに適用できる,ビタビアルゴリズムは全体的な最適解を得ることができる,B
次のうち、線形分類器の最適基準に含まれないものはどれか。,ベイズ分類,感知基準関数,サポートベクターマシン,フィッシャー基準,A
線形回帰において、我々は以下のどの仮定を持つべきですか？,外れ値を見つけることは重要です。なぜなら、線形回帰は外れ値に敏感だからです。,線形回帰はデータが多重共線性を持たないと仮定しています。,線形回帰はすべての変数が正規分布に従う必要があると要求します。,上記のいずれでもない,D
以下は、反復二分木3世代アルゴリズムがデータに求める条件ではありません。,すべての訓練例のすべての属性は明確な値を持たなければなりません,すべての属性は離散量でなければなりません,すべての属性は連続値でなければなりません,同じ要因には同じ結論が得られ、訓練例は一意でなければなりません,C
以下の最適化アルゴリズムの中で、最も高速なのはどれか,BFGS,勾配降下法,ニュートン法,Adam,C
ALBERTに関する次の記述のうち、正しくないものはどれですか。,層間パラメータ共有,単語埋め込みベクトルパラメータの因数分解を採用,下流タスクに適用した場合の予測速度が大幅に向上,ドロップアウトを削除,D
次のうち、どれがSVMの応用例ですか？,新しい記事のクラスタリング,テキストおよびハイパーテキストの分類,画像分類,上記すべて,D
カテゴリ領域界面方程式法において、線形非分離の場合の分類問題の近似解または厳密解を求められない方法はどれか。,二次基準に基づくH-Kアルゴリズム,パーセプトロンアルゴリズム,ポテンシャル関数法,一般逆行列法,B
以下の選択肢の中で、どれが決定論的アルゴリズムに該当しますか。,K-Means,PCA,KNN,以上都不是,B
以下のどのアルゴリズムが、1. KNN；2. 線形回帰；3. ロジスティック回帰。ニューロンネットワークを使って構築できるか。,2 と 3,1 と 2,上記のいずれでもない,"1, 2 と 3",A
SVMの訓練に必要な最小時間計算量はO(n²)であるため、次のうちどのデータセットがSVMに適していないか？,データセットのサイズに関係しない,大規模データセット,小規模データセット,中規模データセット,B
線形回帰モデルのランダム誤差項に不均一分散が存在する場合、通常の最小二乗法によるパラメータ推定量は次のようになります。,不偏であるが、非効率的である,不偏であり、効率的である,準偏であり、非効率的である,準偏であり、効率的である,A
RoBERTaに関する次の記述のうち、正しくないものはどれですか,NSPタスクを行わない,静的マスク機構を採用している,より多くの訓練データを使用する,学習時により大きなバッチサイズを使用する,B
論理回帰の出力と目的値を比較する場合、以下の評価指標の中でどの指標が不適切ですか？,正解率,二乗平均誤差,AUC-ROC,Logloss,B
言語モデルのパラメータ推定は、よくMLE（最尤推定）を使用する。直面する問題の一つとして、出現していない項の確率がゼロになってしまうことがある。これにより、言語モデルの性能が低下してしまう。この問題を解決するために、次のうちどれを使う必要があるか。（ ）,ホワイトノイズを加える,スムージング,ランダム補間,ノイズ除去,B
北京市の人口年齢分布をモデル化する場合、どの分布を使用するのがより適切か。,0-1分布,正規分布,ポアソン分布,指数分布,B
SVMにおけるコストパラメータは次のどれを表していますか：,誤分類とモデルの複雑さのバランス,どれも正しくない,使用されているカーネル,クロスバリデーションの回数,A
SVMの汎化誤差に関する正しい記述はどれですか？,超平面とサポートベクター間の距離,SVMの誤差閾値,上記のいずれでもない,未知のデータに対するSVMの予測能力,D
BERTに関する次の記述のうち、正しくないものはどれですか,意味的な文脈をモデル化できます,活性化関数としてGELUを使用しています,ネットワークは全部で20層あります,Transformerを使用しています,C
パターン認識において、マハラノビス距離がユークリッド距離よりも持つ利点ではないものは次のうちどれか。,スケール不変性,並進不変性,異なる特徴間の関係を考慮する,パターンの分布を考慮する,B
故障が発生する回数を記述する際に、どの分布を使用するのが最も適切か？,0-1分布,指数分布,正規分布,ポアソン分布,D
次のうち、LSTM自身の特徴ではないものはどれか,LSTMはRNNの一種の変形である,勾配消失を防止する,トレーニング時にGPU使用率が高い,LSTMには忘却ゲートがある,C
論理回帰およびサポートベクターマシンに関して誤っている記述はどれか。,論理回帰は本質的にサンプルに基づいて重みを最尤推定する方法である。その後の確率は事前確率と尤度関数の積に比例する。ロジスティック回帰は尤度関数を最大化しているだけで、事後確率を最大化しているわけではない。ましてや事後確率の最小化ではない。,サポートベクターマシンは正則化係数によってモデルの複雑さを制御し、過学習を防ぐことができる。,サポートベクターマシンの目的は、訓練データをできるだけ分離し、かつマージンが最大となる超平面を見つけることであるため、構造的リスク最小化に該当する。,ロジスティック回帰の出力はサンプルが正のクラスに属するオッズであり、確率を計算することが可能である。,A
以下のうち、クラスタリングアルゴリズムの結果に主に影響を与えないものはどれですか？,特徴選択,既知クラスのサンプル品質,分類基準,パターン類似度測定,B
ガウス混合モデル (GMM) はどのような基準を使って訓練されますか？,平均二乗誤差の最小化,経験的リスクの最小化,期待値最大化,上記以外の選択肢,C
次のうち、正しい記述はどれか。,ある機械学習モデルが高精度を持っていても、必ずしもその分類器が優れているとは限らない。,クラスタリングの「クラスID」を新しい特徴項目として使用し、その後で教師あり学習を用いてそれぞれ別に学習することはできない。,モデルの複雑度を増せば、テスト誤り率は常に低下する。,モデルの複雑度を増せば、訓練誤り率は常に低下する。,A
中国語の同義語置換において、Word2Vecは頻繁に使用される。以下の記述の中で誤っているものはどれか。,Word2Vecの結果は現在の予測環境に合致している,Word2Vecによって得られる単語はすべて意味上の同義語である,Word2Vecは訓練用コーパスの量と質に制限される,Word2Vecは確率統計に基づいている,B
以下の異なるシナリオで使用される分析方法のうち、誤っているものはどれか。,メーカーの最近1年間の営業およびサービスデータに基づき、クラスタリングアルゴリズムを使用して天猫の各メーカーがそれぞれ主力商品カテゴリ内で属するショップレベルを判定する,メーカーのここ数年間の成約データに基づき、クラスタリングアルゴリズムを使用してユーザーが次月に発生させる可能性のある消費金額の数式を導き出す,関連ルールアルゴリズムを使用して、車用シートカバーを購入したバイヤーに対して、車用フットマットの推薦が適切かどうかを分析する,ユーザーが最近購入した商品情報をもとに、決定木アルゴリズムを使用して淘宝のバイヤーが男性か女性かを識別する,B
データクリーニングにおいて、次のうちどれが欠損値を処理する方法ではないか。,変数削除,推定,完全ケース削除,ペアワイズ削除,D
LDA（潜在ディリクレ配分）に関する次の記述のうち、誤っているものはどれか。,LDAは非教師あり学習技術である,LDAはEMアルゴリズムの考え方を用いて解くことができる,ある文書を選んだ後、その文書のトピック分布は確定的である,LDAは単語、トピック、文書の3層構造を持っている,C
もしデータセットの全特徴量を使い、100％の精度に達成できたが、テストデータでは約70％までしか達成できない場合、これは次のことを示している：,上記のいずれでもない,アンダーフィッティング,オーバーフィッティング,モデルは非常に優れている,C
SVMを使用してデータXを学習していると仮定します。データXの中には誤りがあるいくつかの点が含まれています。今、二次の多項式カーネル（多項式の次数は2）を使用し、緩やか変数Cをハイパーパラメータの1つとして使用しています。もし大きなC（Cが無限大に向かう）を使用した場合、次のようになります：,以上均不正,不明,正しく分類できない,依然としてデータを正しく分類できる,D
統計に基づく分かち書き方法は,正向最大マッチング法,条件付き確率場,最小分割,逆向最大マッチング法,B
とある学生がナイーブベイズ分類モデルを使用する際に、誤って訓練データの次元を二重にしてしまったと仮定します。このとき、ナイーブベイズに関する記述の中で正しくないものはどれでしょうか。,繰り返し特徴がない場合と比較して、モデルの性能は精度が低下する,繰り返し特徴がない場合と比較して、モデルの性能は精度が向上する,2列の特徴が高度に相関しているときに、2列の特徴が同じである場合に得られる結論を使って問題を分析することはできない,全ての特徴が一度繰り返された場合、繰り返さなかった場合のモデル予測結果と得られたモデルの予測結果は同じになる,D
word2vecに関する次の記述のうち、誤っているものはどれか。,単語ベクトルを使用すると、King - man + woman = Queen という等式が成り立つ,Skip-gramは、与えられた単語ウィンドウ内のテキストから、現在の単語の確率を予測する,word2vecの仮定はBoW（Bag-of-Words）であり、単語の順序は重要ではない,word2vecの訓練では、Negative SamplingとHierarchical Softmaxの二つの高速化アルゴリズムが使用される,B
以下の記述で正しいものはどれか,クラスタリング分析は、一種の非教師あり分類と見なすことができる。,クラスタリング分析において、クラスター内の類似性が大きく、クラスター間の差異が大きい場合、クラスタリングの効果は悪くなる。,SVMはこのような分類器であり、最小マージンの超平面を探すため、しばしば最小マージン分類器とも呼ばれる。,決定木において、ノード数が非常に多くなるにつれて、モデルの訓練誤差が引き続き減少しているにもかかわらず、検証誤差が増加し始める。これはモデルのアンダーフィッティングが発生した問題である。,A
次の選択肢の中で、識別パターンが他と異なるものはどれか。,移動手段の判断：徒歩、自転車、車,ユーザー年齢層の判断：少年、青年、中年、老人,配達員による手紙の仕分け,医師による患者の病型診断,A
次の記述のうち、正しくないものはどれか。,勾配降下法は現在位置の負の勾配を探索方向として用いる方法である,共役勾配法は一階微分の情報のみを利用すればよいが、収束速度が勾配降下法より速い,バッチ勾配降下法と確率的勾配降下法を比較したとき、バッチ勾配降下法の利点は大規模なサンプルに対して効率が高いことである,ニュートン法と勾配降下法を比較したとき、一つの欠点は計算が複雑になることであり、一つの利点は収束速度が速まることである,C
隠しマルコフモデルにおいて、観測列とその観測列を生成する状態列が既知である場合、以下のどの方法を直接的にパラメータ推定に用いることができるか。,前向き後向きアルゴリズム,最尤推定,ビタビアルゴリズム,EMアルゴリズム,B
LDA（潜在ディリクレ配分）において、同一のトピックに属する単語分布の事前分布は何か？,正規分布,ディリクレ分布,多項分布,二項分布,C
線形回帰の基本的な仮定に含まれないものはどれか,説明変数のすべての観測値に対して、誤差項は同じ分散を持つ,誤差項は期待値が0の確率変数である,誤差項は正規分布に従う,誤差項同士は相互に関係している,D
次のうち、SVMのカーネル関数ではないものはどれか。,シグモイドカーネル関数,径方向基底カーネル関数,多項式カーネル関数,ロジスティックカーネル関数,D
以下のうち、教師なし学習の方法はどれですか？,SVM,K-means,KNN,決定木,B
以下のうち、識別モデルに属する手法はどれか。,ベイジアンネットワーク,朴素ベイズ,隠れマルコフモデル,サポートベクターマシン,D
次の交差検証手法において：i. 復元抽出のブートストラップ法；ii. 1つのテストサンプルを除く交差検証；iii. 5分割交差検証；iv. 5分割交差検証を2回繰り返したもの。サンプル数が1000のとき、次の実行時間の順序で正しいものは,ii > iv > iii > i,ii > iii > iv > i,iv > i > ii > iii,i > ii > iii > iv,A
Seq2Seqモデルが復号時に選択可能な方法,グリーディアルゴリズム,どちらも可能,ビームサーチ,どちらも不可能,B
隠れマルコフモデルにおける予測問題を解決するアルゴリズムはどれか。,前向きアルゴリズム,ビタビアルゴリズム,Baum-Welchアルゴリズム,後ろ向きアルゴリズム,B
k平均法クラスタリングアルゴリズムに関する正しい説明は次のうちどれか。,クラス数を自動的に識別できる。初期点をランダムに選択して中心点の計算を行うことはない。,クラス数を自動的に識別できない。初期点をランダムに選択して中心点の計算を行うことはない。,クラス数を自動的に識別できない。初期点をランダムに選択して中心点の計算を行う。,クラス数を自動的に識別できる。初期点をランダムに選択して中心点の計算を行う。,C
一般に、k-NN最近傍法は（　）の場合に効果较好である。,サンプルが団状分布している,サンプルが多いが代表性が良くない,サンプルが鎖状分布している,サンプルが少ないが代表性が良い,D
ある刑務所の顔認証入場システムは、入場しようとする人物の身元を識別するために使用される。このシステムは全部で4種類の異なる人物を識別することができる：刑務官、泥棒、給食係、その他。次のうち、どの学習方法がこのような用途に最も適しているか。,多クラス分類問題,二値分類問題,k-センタークラスタリング問題,階層的クラスタリング問題,A
SVD と同じ射影（projection）を得るために、PCA で何をすればよいですか？,データをゼロ平均に変換する,不可能である,データをゼロ最頻値に変換する,データをゼロ中央値に変換する,A
統計的パターン分類の問題において、事前確率が未知の場合には、次のいずれかを使用することができます。,N-P判定,最小最大損失基準,最小損失基準,最小誤判別確率基準,B
以下のうち、テキスト分類に直接使用できない方法はどれですか？,決策木,Kmeans,サポートベクターマシン,KNN,B
