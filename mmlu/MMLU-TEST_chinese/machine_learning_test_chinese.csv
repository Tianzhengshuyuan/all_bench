1,"陈述1：线性回归估计量在所有无偏估计量中具有最小的方差。  
陈述2：AdaBoost组合的分类器所分配的系数α始终为非负数。",正确，正确,错误，错误,正确，错误,错误，正确,D
2,"陈述1：RoBERTa的预训练语料库大约是BERT预训练语料库的10倍。  
陈述2：2018年的ResNeXt通常使用tanh激活函数。",正确，正确,错误，错误,正确，错误,错误，正确,C
3,"陈述1：与逻辑回归模型一样，支持向量机在给定输入示例的情况下，会给出可能标签上的概率分布。  
陈述2：总体而言，当我们从线性核移动到更高阶的多项式核时，我们期望支持向量保持不变。",对，对,错，错,对，错,错，对,B
4,一个机器学习问题包含四个属性和一个类别。这四个属性分别有3、2、2 和 2 个可能的取值。类别有 3 个可能的取值。最大可能有多少个不同的示例？,12,24,48,72,D
5,截至2020年，哪种架构最适合对高分辨率图像进行分类？,convolutional networks,graph networks,fully connected networks,RBF networks,A
6,"陈述1：在期望最大化算法的连续迭代过程中，数据的对数似然值总会增加。  
陈述2：Q学习的一个缺点是它只能在学习者事先知道其行为如何影响环境的情况下使用。",正确，正确,错误，错误,正确，错误,错误，正确,B
7,假设我们已经计算出了代价函数的梯度，并将其存储在一个向量 g 中。在给定梯度的情况下，一次梯度下降更新的代价是多少？,O(D),O(N),O(ND),O(ND^2),A
8,"陈述1：对于一个连续型随机变量x及其概率密度函数p(x)，对于所有x，都有0 ≤ p(x) ≤ 1。  
陈述2：决策树是通过最小化信息增益来学习的。",正确，正确,错误，错误,正确，错误,错误，正确,B
9,考虑下面给出的贝叶斯网络。对于这个贝叶斯网络 H -> U <- P <- W，需要多少个独立参数？,2,4,8,16,C
10,随着训练样本数量趋于无穷大，基于这些数据训练出的模型将会具有：,更低的方差,更高的方差,相同的方差,以上都不是,A
11,"陈述1：在二维平面中所有矩形的集合（包括非轴对齐的矩形）可以打散5个点的集合。  
陈述2：当k = 1时，k-近邻分类器的VC维是无限的。","True, True","False, False","True, False","False, True",A
12,_ 指的是既无法对训练数据进行建模，也无法推广到新数据的模型。,good fitting,overfitting,underfitting,all of the above,C
13,"陈述1：F1分数对于类别高度不平衡的数据集尤其有用。  
陈述2：ROC曲线下的面积是用于评估异常检测器的主要指标之一。",正确，正确,错误，错误,正确，错误,错误，正确,A
14,"陈述1：反向传播算法可以学习到具有隐藏层的全局最优神经网络。  
陈述2：一条直线的VC维应该至多为2，因为我可以找到至少一个3个点的情况，无法被任何直线打碎（正确分类）。",正确，正确,错误，错误,正确，错误,错误，正确,B
15,高熵意味着分类中的划分是,纯的,不纯的,有用的,无用的,B
16,"陈述1：在原始ResNet论文中使用的是层归一化（Layer Normalization），而不是批归一化（Batch Normalization）。  
陈述2：DCGANs使用自注意力（self-attention）来稳定训练过程。","True, True","False, False","True, False","False, True",B
17,在为特定数据集构建线性回归模型时，你观察到其中一个特征的系数具有相对较大的负值。这表明,该特征对模型有显著影响（应保留）,该特征对模型没有显著影响（应忽略）,在没有额外信息的情况下，无法评论该特征的重要性,无法确定任何结论。,C
18,对于一个神经网络，以下哪一个结构假设最影响欠拟合（即高偏差模型）和过拟合（即高方差模型）之间的权衡：,隐藏节点的数量,学习率,权重的初始选择,使用常数项单元输入,A
19,对于多项式回归，以下哪一个结构假设最影响欠拟合和过拟合之间的权衡：,多项式阶数,我们是通过矩阵求逆还是梯度下降来学习权重,高斯噪声的假设方差,使用常数项单位输入,A
20,"陈述1：截至2020年，某些模型在CIFAR-10上的准确率超过98%。  
陈述2：原始的ResNet并未使用Adam优化器进行优化。","True, True","False, False","True, False","False, True",A
21,K均值算法：,要求特征空间的维度不超过样本数量,当K = 1时目标函数值最小,对于给定的聚类数量，最小化类内方差,当且仅当初始均值选择为某些样本本身时，才会收敛到全局最优解,C
22,"陈述1：VGGNets 的卷积核宽度和高度都小于 AlexNet 的第一层卷积核。  
陈述2：在 Batch Normalization 之前引入了依赖于数据的权重初始化方法。",正确，正确,错误，错误,正确，错误,错误，正确,A
23,"下列矩阵的秩是多少？A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]",0,1,2,3,B
24,"陈述1：密度估计（例如使用核密度估计器）可用于执行分类。  
陈述2：逻辑回归与高斯朴素贝叶斯（具有相同的类协方差矩阵）之间的对应关系意味着这两个分类器的参数之间存在一一对应关系。","True, True","False, False","True, False","False, True",C
25,假设我们想要对空间数据（例如房屋的几何位置）进行聚类分析。我们希望生成许多不同大小和形状的聚类。以下哪种方法最合适？,决策树,基于密度的聚类,基于模型的聚类,K均值聚类,B
26,"陈述1：在AdaBoost中，被错误分类样本的权重会以相同的乘法因子增加。  
陈述2：在AdaBoost中，第t个弱分类器在带有权重D_t的训练数据上的加权训练误差e_t通常随着t的增加而增大。","True, True","False, False","True, False","False, True",A
27,MLE估计通常不受欢迎，因为,它们是有偏的,它们的方差很高,它们不是一致估计量,以上都不是,B
28,梯度下降的计算复杂度是：,线性于D,线性于N,D的多项式,取决于迭代次数,C
29,对多个决策树的输出进行平均有助于 _ 。,增加偏差,减少偏差,增加方差,减少方差,D
30,通过对已识别特征子集应用线性回归得到的模型可能不同于在识别子集的过程中最终得到的模型，这种情况发生在以下哪些方法中？,最优子集选择,向前逐步选择,向前阶段选择,以上所有方法,C
31,神经网络：,优化一个凸目标函数,只能使用随机梯度下降进行训练,可以混合使用不同的激活函数,以上都不是,C
32,已知某种疾病 D 的发病率约为每 100 人中有 5 例（即 P(D) = 0.05）。设布尔随机变量 D 表示“患有疾病 D”，布尔随机变量 TP 表示“检测结果为阳性”。这种疾病 D 的检测手段被认为是非常准确的，具体表现为：当你患有疾病 D 时，检测结果为阳性的概率为 0.99；而当你没有患疾病 D 时，检测结果为阴性的概率为 0.97。那么 P(TP)，即检测结果为阳性的先验概率是多少？,0.0368,0.473,0.078,None of the above,C
33,"陈述1：通过径向基函数核映射到特征空间Q后，使用未加权欧氏距离的1-NN可能能够在分类性能上比在原始空间中更好（尽管我们无法保证这一点）。  
陈述2：感知机的VC维小于简单线性支持向量机的VC维。","True, True","False, False","True, False","False, True",B
34,网格搜索的缺点是,无法应用于不可导函数。,无法应用于不连续函数。,难以实现。,在多元线性回归中运行速度较慢。,D
35,根据各种线索预测一个地区的降雨量是一个______问题。,监督学习,无监督学习,聚类,以上都不是,A
36,关于回归分析，以下哪句话是错误的？,它将输入与输出相关联。,它用于预测。,它可以用于解释。,它能够发现因果关系,D
37,以下哪一项是剪枝决策树的主要原因？,为了在测试期间节省计算时间,为了节省存储决策树的空间,为了使训练集误差更小,为了避免对训练集过拟合,D
38,"陈述1：核密度估计等同于在原始数据集中的每个点Xi处使用值Yi = 1/n进行核回归。  
陈述2：学习得到的决策树的深度可以大于创建该树所用的训练样本数量。",正确，正确,错误，错误,正确，错误,错误，正确,B
39,假设你的模型出现了过拟合。以下哪一项不是有效的尝试来减少过拟合？,增加训练数据的数量。,改进用于误差最小化的优化算法。,降低模型的复杂度。,减少训练数据中的噪声。,B
40,陈述1：softmax函数通常用于多类逻辑回归中。陈述2：非均匀softmax分布的温度会影响其熵值。,正确，正确,错误，错误,正确，错误,错误，正确,A
41,关于支持向量机（SVM），以下哪项是正确的？,对于二维数据点，线性SVM学习到的分离超平面将是一条直线。,从理论上讲，高斯核SVM无法建模任何复杂的分离超平面。,对于SVM中使用的每一个核函数，都可以获得一个等效的闭合形式的基展开。,过拟合并不是SVM中支持向量数量的函数。,A
42,以下哪一项是给定贝叶斯网络 H -> U <- P <- W 所描述的 H、U、P 和 W 的联合概率？[注：以条件概率的乘积形式表示],"P(H, U, P, W) = P(H) * P(W) * P(P) * P(U)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(W | H, P)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(U | H, P)",以上都不是,C
43,"陈述1：由于使用径向基核的SVM的VC维是无限的，因此这样的SVM一定比具有有限VC维的多项式核SVM效果差。  
陈述2：一个具有线性激活函数的两层神经网络本质上是给定数据集上训练出的线性分类器的加权组合；基于线性分类器的提升算法同样会找到线性分类器的组合，因此这两种算法将给出相同的结果。",正确，正确,错误，错误,正确，错误,错误，正确,B
44,"陈述1：ID3算法可以保证找到最优决策树。  
陈述2：考虑一个连续概率分布，其密度函数f()处处非零。则某个值x的概率等于f(x)。","True, True","False, False","True, False","False, True",B
45,给定一个具有 N 个输入节点、没有隐藏层、一个输出节点，并使用熵损失函数和 Sigmoid 激活函数的神经网络，以下哪种算法（具有适当的超参数和初始化）可以用于找到全局最优解？,随机梯度下降,小批量梯度下降,批量梯度下降,以上所有算法,D
46,在以下选项中选择最可能的选项：,减少模型偏差,减少估计偏差,减少方差,对偏差和方差没有影响,A
47,考虑下面给出的贝叶斯网络。如果我们不对独立性或条件独立性做出任何假设（即 H -> U <- P <- W），我们需要多少个独立参数？,3,4,7,15,D
48,另一个用于分布外检测的术语是？,异常检测,单类检测,训练-测试不匹配鲁棒性,背景检测,A
49,"陈述1：我们通过提升弱学习器h来学习一个分类器f。f的决策边界函数形式与h相同，但参数不同（例如，如果h是一个线性分类器，那么f也是一个线性分类器）。  
陈述2：交叉验证可用于选择提升算法中的迭代次数；该过程可能有助于减少过拟合。",正确，正确,错误，错误,正确，错误,错误，正确,D
50,"陈述1：高速公路网络是在ResNets之后引入的，且为了避免使用最大池化而倾向于使用卷积。  
陈述2：DenseNets通常比ResNets消耗更多的内存。",正确，正确,错误，错误,正确，错误,错误，正确,D
51,若N是训练数据集中的实例数量，则最近邻算法的分类运行时间为,O(1),O( N ),O(log N ),O( N^2 ),B
52,"陈述1：原始ResNet和Transformer都是前馈神经网络。  
陈述2：原始Transformer使用自注意力机制，但原始ResNet不使用。","True, True","False, False","True, False","False, True",A
53,"陈述1：RELU函数不是单调的，但Sigmoid函数是单调的。  
陈述2：使用梯度下降训练的神经网络以高概率收敛到全局最优解。",正确，正确,错误，错误,正确，错误,错误，正确,D
54,神经网络中sigmoid节点的数值输出：,无界，包含所有实数。,无界，包含所有整数。,有界，在0和1之间。,有界，在-1和1之间。,C
55,以下哪项只能在训练数据线性可分时使用？,Linear hard-margin SVM.,Linear Logistic Regression.,Linear Soft margin SVM.,The centroid method.,A
56,以下哪些是空间聚类算法？,基于划分的聚类,K-均值聚类,基于网格的聚类,以上全部,D
57,"陈述1：支持向量机构建的最大边缘决策边界在所有线性分类器中具有最低的泛化误差。  
陈述2：原则上，通过使用次数小于或等于三次的多项式核函数的支持向量机（SVM），可以复现从带有类别条件高斯分布的生成模型得到的任何决策边界。","True, True","False, False","True, False","False, True",D
58,"陈述1：线性模型的L2正则化通常比L1正则化使模型更加稀疏。  
陈述2：残差连接可以在ResNet和Transformer中找到。",正确，正确,错误，错误,正确，错误,错误，正确,D
59,"假设我们想要计算 P(H|E, F)，并且我们没有条件独立性的信息。以下哪一组数值足以用于该计算？","P(E, F), P(H), P(E|H), P(F|H)","P(E, F), P(H), P(E, F|H)","P(H), P(E|H), P(F|H)","P(E, F), P(E|H), P(F|H)",B
60,以下哪项在执行bagging时可以防止过拟合？,使用有放回抽样作为抽样技术,使用弱分类器,使用不易过拟合的分类算法,对每个训练的分类器进行验证的做法,B
61,"陈述1：PCA和谱聚类（例如Andrew Ng的谱聚类）在两个不同的矩阵上进行特征分解。然而，这两个矩阵的大小是相同的。  
陈述2：由于分类是回归的一种特殊情况，因此逻辑回归是线性回归的一种特殊情况。","True, True","False, False","True, False","False, True",B
62,"陈述1：斯坦福情感树库包含的是电影评论，而不是书评。  
陈述2：宾州树库已被用于语言建模。",正确，正确,错误，错误,正确，错误,错误，正确,A
63,"以下矩阵的零空间的维数是多少？A = [[3, 2, −9], [−6, −4, 18], [12, 8, −36]]",0,1,2,3,C
64,什么是支持向量？,距离决策边界最远的样本。,在SVM中计算f(x)所需的唯一样本。,数据质心。,所有在SVM中具有非零权重αk的样本。,B
65,"陈述1：Word2Vec参数不是使用受限玻尔兹曼机初始化的。  
陈述2：tanh函数是一个非线性激活函数。",正确，正确,错误，错误,正确，错误,错误，正确,A
66,如果你的训练损失随着训练轮数的增加而增加，那么以下哪一项可能是学习过程中存在问题的可能原因？,正则化太低，模型过拟合,正则化太高，模型欠拟合,步长太大,步长太小,C
67,已知疾病D的发病率约为每100人中有5例（即P(D) = 0.05）。设布尔随机变量D表示“患有疾病D”，布尔随机变量TP表示“检测结果为阳性”。已知这种检测疾病D的方法非常准确，准确率表现为：当你患有疾病D时检测为阳性的概率是0.99，而当你未患疾病D时检测为阴性的概率是0.97。那么当检测结果为阳性时，你确实患有疾病D的后验概率P(D | TP)是多少？,0.0495,0.078,0.635,0.97,C
68,陈述1：传统机器学习的结果假设训练集和测试集是独立同分布的。陈述2：2017年，COCO模型通常在ImageNet上进行预训练。,"True, True","False, False","True, False","False, True",A
69,"陈述1：在相同的训练集上，由两种不同的核函数K1(x, x0)和K2(x, x0)得到的间隔（margins）值并不能告诉我们哪一个分类器在测试集上的表现会更好。  
陈述2：BERT的激活函数是GELU。","True, True","False, False","True, False","False, True",A
70,以下哪项是机器学习中的聚类算法？,Expectation Maximization,CART,Gaussian Naïve Bayes,Apriori,A
71,你刚刚完成了一个用于垃圾邮件分类的决策树训练，但它在训练集和测试集上的表现都非常差。你知道你的实现没有bug，那么可能是什么原因导致这个问题？,Your decision trees are too shallow.,You need to increase the learning rate.,You are overfitting.,None of the above.,A
72,K折交叉验证的计算复杂度是,与K成线性关系,与K成二次关系,与K成三次关系,与K成指数关系,A
73,"陈述1：工业级的神经网络通常是在CPU而非GPU上进行训练的。  
陈述2：ResNet-50模型拥有超过10亿个参数。",正确，正确,错误，错误,正确，错误,错误，正确,B
74,已知两个布尔随机变量 A 和 B，其中 P(A) = 1/2，P(B) = 1/3，且 P(A | ¬B) = 1/4，那么 P(A | B) 是多少？,1/6,1/4,3/4,1,D
75,人工智能所带来的存在性风险最常与以下哪位教授相关联？,Nando de Frietas,Yann LeCun,Stuart Russell,Jitendra Malik,C
76,"陈述1：最大化逻辑回归模型的似然函数会产生多个局部最优解。  
陈述2：如果数据的分布是已知的，没有任何分类器能优于朴素贝叶斯分类器。","True, True","False, False","True, False","False, True",B
77,对于核回归来说，以下哪一个结构假设对欠拟合与过拟合之间的权衡影响最大：,核函数是高斯型、三角型还是盒型,我们使用欧几里得距离、L1距离还是L∞距离,核宽度,核函数的最大高度,C
78,"陈述1：SVM学习算法可以保证找到相对于其目标函数的全局最优假设。  
陈述2：通过径向基核函数映射到特征空间Q后，感知机可能能够比在原始空间中获得更好的分类性能（尽管我们无法保证这一点）。","True, True","False, False","True, False","False, True",A
79,对于高斯贝叶斯分类器，以下哪一个结构假设最影响欠拟合与过拟合之间的权衡：,我们是通过最大似然还是梯度下降来学习类别中心,我们假设每个类别具有完整的协方差矩阵还是对角协方差矩阵,我们的类别先验概率是否相等，还是通过数据估计得到,我们是否允许类别具有不同的均值向量，还是强制它们共享相同的均值向量,B
80,"陈述1：当训练数据集较小时，更容易发生过拟合。  
陈述2：当假设空间较小时，更容易发生过拟合。","True, True","False, False","True, False","False, True",D
81,"陈述1：除了EM算法，梯度下降也可用于高斯混合模型的推断或学习。  
陈述2：在属性数量固定的情况下，可以在与数据集中记录数量成线性关系的时间内学习一个基于高斯的贝叶斯最优分类器。","True, True","False, False","True, False","False, True",A
82,"陈述1：在贝叶斯网络中，联合树算法的推理结果与变量消元法的推理结果相同。  
陈述2：如果两个随机变量 X 和 Y 在给定另一个随机变量 Z 的条件下是条件独立的，那么在相应的贝叶斯网络中，X 和 Y 的节点在给定 Z 的情况下是d-分离的。","True, True","False, False","True, False","False, True",C
83,给定一个包含大量心脏病患者医疗记录的数据集，尝试学习是否存在不同的患者群组，以便针对不同群组制定不同的治疗方法。这是一个什么样的学习问题？,监督学习,无监督学习,以上两者都是,以上都不是,B
84,在PCA中，你需要做什么才能得到与SVD相同的投影？,将数据转换为零均值,将数据转换为零中位数,不可能,以上都不是,A
85,"陈述1：1-近邻分类器的训练误差为0。  
陈述2：随着数据点数量趋于无穷大，对于所有可能的先验分布，MAP估计会趋近于MLE估计。换句话说，给定足够的数据，先验分布的选择不再重要。",正确，正确,错误，错误,正确，错误,错误，正确,C
86,在使用正则化的最小二乘回归中（假设优化可以精确完成），增加正则化参数 λ 时，测试误差会如何变化？,将永远不会减少训练误差。,将永远不会增加训练误差。,将永远不会减少测试误差。,将永远不会增加,A
87,以下哪一项最准确地描述了判别方法（discriminative approaches）试图建模的内容？（w 是模型中的参数）,"p(y|x, w)","p(y, x)","p(w|x, w)",以上都不是,A
88,"陈述1 | 卷积神经网络在CIFAR-10分类任务上的性能可以超过95%。  
陈述2 | 神经网络的集成并不能提高分类准确率，因为它们学习到的表示高度相关。",正确，正确,错误，错误,正确，错误,错误，正确,C
89,下列哪一点是贝叶斯学派和频率学派会意见不一致的？,在概率回归中使用非高斯噪声模型。,使用概率模型进行回归。,在概率模型中对参数使用先验分布。,在高斯判别分析中使用类别先验。,C
90,"陈述1：BLEU指标使用精确率，而ROGUE指标使用召回率。  
陈述2：隐马尔可夫模型经常被用来建模英文句子。",正确，正确,错误，错误,正确，错误,错误，正确,A
91,陈述1：ImageNet 包含各种分辨率的图像。陈述2：Caltech-101 的图像数量超过 ImageNet。,正确，正确,错误，错误,正确，错误,错误，正确,C
92,以下哪项更适合进行特征选择？,Ridge,Lasso,以上两者,以上都不是,B
93,假设你得到了一个EM算法，用于寻找具有潜在变量的模型的最大似然估计。现在你需要修改该算法，使其寻找最大后验估计（MAP）而不是最大似然估计。你需要修改哪个或哪些步骤？,期望步,最大化步,无需修改,两者都需要,B
94,对于高斯贝叶斯分类器，以下哪一个结构假设最影响欠拟合和过拟合之间的权衡：,我们是通过最大似然还是梯度下降来学习类别中心,我们假设每个类别具有完整的协方差矩阵还是对角协方差矩阵,我们是否具有相等的类别先验概率，还是通过数据估计得到的先验概率,我们是否允许类别具有不同的均值向量，还是强制它们共享相同的均值向量,B
95,"陈述1：对于任何两个具有联合分布p(x, y)的变量x和y，我们总有H[x, y] ≥ H[x] + H[y]，其中H是熵函数。  
陈述2：对于某些有向图，道德化（moralization）会减少图中存在的边的数量。",正确，正确,错误，错误,正确，错误,错误，正确,B
96,以下哪项不是监督学习？,PCA,决策树,线性回归,朴素贝叶斯,A
97,陈述1：神经网络的收敛性取决于学习率。陈述2：Dropout会将随机选择的激活值乘以零。,正确，正确,错误，错误,正确，错误,错误，正确,A
98,"以下哪一个等于给定布尔随机变量 A、B 和 C（它们之间没有任何独立性或条件独立性假设）的 P(A, B, C)？",P(A | B) * P(B | C) * P(C | A),"P(C | A, B) * P(A) * P(B)","P(A, B | C) * P(C)","P(A | B, C) * P(B | A, C) * P(C | A, B)",C
99,以下哪项任务最适合使用聚类方法来解决。,根据各种线索预测降雨量,检测欺诈性信用卡交易,训练机器人解决迷宫问题,以上所有,B
100,在线性回归中应用了正则化惩罚后，你发现w的一些系数变为零。以下哪一种惩罚可能被使用了？,L0 范数,L1 范数,L2 范数,(a) 或 (b) 中的一个,D
101,"A 和 B 是两个事件。如果 P(A, B) 减小，而 P(A) 增加，以下哪一项是正确的？",P(A|B) 减小,P(B|A) 减小,P(B) 减小,以上全部,B
102,"陈述1：在为固定数量的观测值学习HMM时，假设我们不知道隐藏状态的真实数量（这通常是实际情况），通过允许更多的隐藏状态，我们总能提高训练数据的可能性。  
陈述2：协同过滤通常是建模用户电影偏好的一个有用模型。","True, True","False, False","True, False","False, True",A
103,你正在训练一个用于简单估计任务的线性回归模型，并注意到模型对数据存在过拟合现象。于是你决定加入ℓ₂正则化来惩罚权重。当你增加ℓ₂正则化系数时，模型的偏差和方差会发生什么变化？,偏差增加；方差增加,偏差增加；方差减少,偏差减少；方差增加,偏差减少；方差减少,B
104,"以下哪个 PyTorch 1.8 命令可以生成一个 $10\times 5$ 的高斯矩阵，其中每个元素都是从 $\mathcal{N}(\mu=5,\sigma^2=16)$ 独立同分布采样得到的，以及一个 $10\times 10$ 的均匀矩阵，其中每个元素都是从 $U[-1,1)$ 独立同分布采样得到的？","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{torch.rand(10,10,low=-1,high=1)}","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{(torch.rand(10,10) - 0.5) / 0.5}","\texttt{5 + torch.randn(10,5) * 4} ; \texttt{2 * torch.rand(10,10) - 1}","\texttt{torch.normal(torch.ones(10,5)*5,torch.ones(5,5)*16)} ; \texttt{2 * torch.rand(10,10) - 1}",C
105,"陈述1：ReLU在x<0时梯度为零，而Sigmoid的梯度σ(x)(1-σ(x))对所有x来说都小于等于1/4。  
陈述2：Sigmoid具有连续的梯度，而ReLU的梯度是不连续的。","True, True","False, False","True, False","False, True",A
106,关于批归一化（Batch Normalization），以下哪项是正确的？,应用批归一化之后，层的激活值将遵循标准高斯分布。,如果在仿射层（affine layer）之后立即使用批归一化层，仿射层的偏置参数将变得多余。,在使用批归一化时，必须改变标准的权重初始化方法。,对于卷积神经网络，批归一化等同于层归一化（Layer Normalization）。,B
107,"假设我们有以下目标函数：
$$\argmin_{w} \frac{1}{2} \|Xw-y\|^2_2 + \frac{1}{2}\gamma \|w\|^2_2$$

那么，$\frac{1}{2} \|Xw-y\|^2_2 + \frac{1}{2}\lambda \|w\|^2_2$ 关于 $w$ 的梯度是什么？",$\nabla_w f(w) = (X^\top X + \lambda I)w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + (\lambda+1) w$,C
108,以下关于卷积核的说法哪一个是正确的？,用$\begin{bmatrix}1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$对图像进行卷积不会改变图像,用$\begin{bmatrix}0 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}$对图像进行卷积不会改变图像,用$\begin{bmatrix}1 & 1 & 1\\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}$对图像进行卷积不会改变图像,用$\begin{bmatrix}0 & 0 & 0\\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}$对图像进行卷积不会改变图像,B
109,以下哪一项是错误的？,语义分割模型预测每个像素的类别，而多类别图像分类器预测整张图像的类别。,一个具有96% IoU（交并比）的边界框很可能会被视为真阳性（True Positive）。,当一个预测出的边界框在场景中没有对应任何物体时，它会被视为假阳性（False Positive）。,一个具有3% IoU（交并比）的边界框很可能会被视为假阴性（False Negative）。,D
110,以下哪一项是错误的？,没有激活函数的以下全连接网络是线性的：$g_3(g_2(g_1(x)))$，其中 $g_i(x) = W_i x$ 且 $W_i$ 是矩阵。,"Leaky ReLU $\max\{0.01x,x\}$ 是凸函数。",ReLU 的组合如 $ReLU(x) - ReLU(x-1)$ 是凸函数。,损失函数 $\log \sigma(x)= -\log(1+e^{-x})$ 是凹函数。,C
111,我们正在训练一个具有两个隐藏层的全连接网络来预测房价。输入为100维，包含若干特征，例如平方英尺数、家庭收入中位数等。第一个隐藏层有1000个激活单元，第二个隐藏层有10个激活单元。输出是一个标量，表示房屋价格。假设该网络为普通的网络结构，使用仿射变换，且激活函数中没有批归一化，也没有可学习的参数。那么该网络共有多少个参数？,111021,110010,111110,110011,A
112,"陈述1：Sigmoid函数$\sigma(x)=(1+e^{-x})^{-1}$关于$x$的导数等于$\text{Var}(B)$，其中$B\sim \text{Bern}(\sigma(x))$是一个伯努利随机变量。  
陈述2：将神经网络每一层的偏置参数设为0会改变偏差-方差的权衡，使得模型的方差增加，而模型的偏差减小。","True, True","False, False","True, False","False, True",C
